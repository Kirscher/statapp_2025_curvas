{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.utils import column_or_1d\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 953)\n"
     ]
    }
   ],
   "source": [
    "img = nib.load('UKCHLL_001_000.nii.gz')\n",
    "\n",
    "data = img.get_fdata()\n",
    "\n",
    "print(data.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16000000000000003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' ils font juste nombre de ood trouvés/nb ood mais la manière dont est défini le OOD est peu claire et semble arbitraire'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AUROC\n",
    "\"\"\" y_true=true labels et y_score= score d'incertitude\"\"\"\n",
    "y_true = [1,0,1,1,0,1,0,0,0,1]\n",
    "y_score = [0.1, 0.4, 0.35, 0.8, 0.7, 0.2, 0.9, 0.6, 0.8, 0.4]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(roc_auc)\n",
    "#OOD detection rate\n",
    "\"\"\" ils font juste nombre de ood trouvés/nb ood mais la manière dont est défini le OOD est peu claire et semble arbitraire\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75 0.6\n"
     ]
    }
   ],
   "source": [
    "#DICE et IoU\n",
    "#exemple\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0]\n",
    "y_pred = [1, 0, 1, 0, 0, 1, 1]\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "dice=2 * tp / (2 * tp + fp + fn)\n",
    "iou=tp / (tp + fp + fn)\n",
    "\n",
    "print(dice,iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.25)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ACE\n",
    "#correct=1 si y_true=y_pred et confid=confiance du modèle\n",
    "\n",
    "def calib_stats(correct, calib_confids):\n",
    "    # calib_confids = np.clip(self.confids, 0, 1)\n",
    "    n_bins = 20\n",
    "    y_true = column_or_1d(correct)\n",
    "    y_prob = column_or_1d(calib_confids)\n",
    "\n",
    "    if y_prob.min() < 0 or y_prob.max() > 1:\n",
    "        raise ValueError(\n",
    "            \"y_prob has values outside [0, 1] and normalize is \" \"set to False.\"\n",
    "        )\n",
    "\n",
    "    labels = np.unique(y_true)\n",
    "    if len(labels) > 2:\n",
    "        raise ValueError(\n",
    "            \"Only binary classification is supported. \" f\"Provided labels {labels}.\"\n",
    "        )\n",
    "    y_true = label_binarize(y_true, classes=labels)[:, 0]\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0 + 1e-8, n_bins + 1)\n",
    "\n",
    "    binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "    bin_sums = np.bincount(binids, weights=y_prob, minlength=len(bins))\n",
    "    bin_true = np.bincount(binids, weights=y_true, minlength=len(bins))\n",
    "    bin_total = np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "    nonzero = bin_total != 0\n",
    "    num_nonzero = len(nonzero[nonzero == True])\n",
    "    prob_true = bin_true[nonzero] / bin_total[nonzero]\n",
    "    prob_pred = bin_sums[nonzero] / bin_total[nonzero]\n",
    "    prob_total = bin_total[nonzero] / bin_total.sum()\n",
    "\n",
    "    bin_discrepancies = np.abs(prob_true - prob_pred)\n",
    "    return bin_discrepancies, prob_total, num_nonzero\n",
    "\n",
    "\n",
    "def calc_ace(correct, calib_confids):\n",
    "    bin_discrepancies, _, num_nonzero = calib_stats(correct, calib_confids)\n",
    "    return (1 / num_nonzero) * np.sum(bin_discrepancies)\n",
    "\n",
    "#exemple\n",
    "correct= [0, 1, 0, 1, 0, 1, 1, 0, 0, 1]\n",
    "calib_confids= [0.1, 0.9, 0.4, 0.8, 0.2, 0.6, 0.7, 0.3, 0.4, 0.9]\n",
    "calc_ace(correct,calib_confids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8932699455139714)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NCC\n",
    "def compute_ncc(gt_unc_map: np.array, pred_unc_map: np.array):\n",
    "    \"\"\"\n",
    "    Compute the normalized cross correlation between a ground truth uncertainty and a predicted uncertainty map,\n",
    "    to determine how similar the maps are.\n",
    "    :param gt_unc_map: the ground truth uncertainty map based on the rater variability\n",
    "    :param pred_unc_map: the predicted uncertainty map\n",
    "    :return: float: the normalized cross correlation between gt and predicted uncertainty map\n",
    "    \"\"\"\n",
    "    mu_gt = np.mean(gt_unc_map)\n",
    "    mu_pred = np.mean(pred_unc_map)\n",
    "    sigma_gt = np.std(gt_unc_map, ddof=1)\n",
    "    sigma_pred = np.std(pred_unc_map, ddof=1)\n",
    "    gt_norm = gt_unc_map - mu_gt\n",
    "    pred_norm = pred_unc_map - mu_pred\n",
    "    prod = np.sum(np.multiply(gt_norm, pred_norm))\n",
    "    ncc = (1 / (np.size(gt_unc_map) * sigma_gt * sigma_pred)) * prod\n",
    "    return ncc\n",
    "\n",
    "#exemple\n",
    "gt_unc_map = np.array([\n",
    "    [0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "    [0.3, 0.3, 0.5, 0.6, 0.7],\n",
    "    [0.4, 0.4, 0.6, 0.7, 0.8],\n",
    "    [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "])\n",
    "\n",
    "pred_unc_map = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    [0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "    [0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "    [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "])\n",
    "\n",
    "compute_ncc(gt_unc_map,pred_unc_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2708333333333333\n",
      "0.0625\n"
     ]
    }
   ],
   "source": [
    "#AURC et EAURC\n",
    "\"\"\"\n",
    "risk=array avec erreurs/loss pour chaque image\n",
    "confids=confiance dans la préd\n",
    "\"\"\"\n",
    "\n",
    "def rc_curve_stats(\n",
    "    risks: np.array, confids: np.array\n",
    ") -> tuple[list[float], list[float], list[float]]:\n",
    "    coverages = []\n",
    "    selective_risks = []\n",
    "    assert (\n",
    "        len(risks.shape) == 1 and len(confids.shape) == 1 and len(risks) == len(confids)\n",
    "    )\n",
    "\n",
    "    n_samples = len(risks)\n",
    "    idx_sorted = np.argsort(confids)\n",
    "\n",
    "    coverage = n_samples\n",
    "    error_sum = sum(risks[idx_sorted])\n",
    "\n",
    "    coverages.append(coverage / n_samples)\n",
    "    selective_risks.append(error_sum / n_samples)\n",
    "\n",
    "    weights = []\n",
    "\n",
    "    tmp_weight = 0\n",
    "    for i in range(0, len(idx_sorted) - 1):\n",
    "        coverage = coverage - 1\n",
    "        error_sum = error_sum - risks[idx_sorted[i]]\n",
    "        tmp_weight += 1\n",
    "        if i == 0 or confids[idx_sorted[i]] != confids[idx_sorted[i - 1]]:\n",
    "            coverages.append(coverage / n_samples)\n",
    "            selective_risks.append(error_sum / (n_samples - 1 - i))\n",
    "            weights.append(tmp_weight / n_samples)\n",
    "            tmp_weight = 0\n",
    "    return coverages, selective_risks, weights\n",
    "\n",
    "def aurc(risks: np.array, confids: np.array):\n",
    "    _, risks, weights = rc_curve_stats(risks, confids)\n",
    "    return sum(\n",
    "        [(risks[i] + risks[i + 1]) * 0.5 * weights[i] for i in range(len(weights))]\n",
    "    )\n",
    "\n",
    "def eaurc(risks: np.array, confids: np.array):\n",
    "    \"\"\"Compute normalized AURC, i.e. subtract AURC of optimal CSF (given fixed risks).\"\"\"\n",
    "    n = len(risks)\n",
    "    # optimal confidence sorts risk. Asencding here because we start from coverage 1/n\n",
    "    selective_risks = np.sort(risks).cumsum() / np.arange(1, n + 1)\n",
    "    aurc_opt = selective_risks.sum() / n\n",
    "    return aurc(risks, confids) - aurc_opt\n",
    "\n",
    "\n",
    "#exemple\n",
    "confids = np.array([0.9, 0.8, 0.3, 0.6])  \n",
    "\n",
    "risks = np.array([0, 1, 1, 0]) \n",
    "print(aurc(risks,confids))\n",
    "print(eaurc(risks,confids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNC MAP (mais très emmêlé) --> utile ?\n",
    "def get_gt_unc_map(self, image_id):\n",
    "        if self.exp_version.gt_unc_map_loading is None:\n",
    "            n_reference_segs = self.exp_version.n_reference_segs\n",
    "            reference_segs_paths = [\n",
    "                self.ref_seg_dir / f\"{image_id}_{i:02d}{self.exp_version.image_ending}\"\n",
    "                for i in range(n_reference_segs)\n",
    "            ]\n",
    "            reference_segs = []\n",
    "            for reference_seg_path in reference_segs_paths:\n",
    "                reference_seg, _ = load(reference_seg_path)\n",
    "                reference_segs.append(reference_seg)\n",
    "            reference_segs = np.array(reference_segs)\n",
    "            per_pixel_variance = np.var(reference_segs, axis=0)\n",
    "        else:\n",
    "            per_pixel_variance = hydra.utils.instantiate(\n",
    "                self.exp_version.gt_unc_map_loading,\n",
    "                image_id=image_id,\n",
    "                dataloader=self.dataloader,\n",
    "            )\n",
    "        return per_pixel_variance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUROC\n",
    "\"\"\" y_true=true labels et y_score= score d'incertitude\"\"\"\n",
    "fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "#OOD detection rate\n",
    "\"\"\" ils font juste nombre de ood trouvés/nb ood mais la manière dont est défini le OOD est peu claire et semble arbitraire\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NCC\n",
    "def compute_ncc(gt_unc_map: np.array, pred_unc_map: np.array):\n",
    "    \"\"\"\n",
    "    Compute the normalized cross correlation between a ground truth uncertainty and a predicted uncertainty map,\n",
    "    to determine how similar the maps are.\n",
    "    :param gt_unc_map: the ground truth uncertainty map based on the rater variability\n",
    "    :param pred_unc_map: the predicted uncertainty map\n",
    "    :return: float: the normalized cross correlation between gt and predicted uncertainty map\n",
    "    \"\"\"\n",
    "    mu_gt = np.mean(gt_unc_map)\n",
    "    mu_pred = np.mean(pred_unc_map)\n",
    "    sigma_gt = np.std(gt_unc_map, ddof=1)\n",
    "    sigma_pred = np.std(pred_unc_map, ddof=1)\n",
    "    gt_norm = gt_unc_map - mu_gt\n",
    "    pred_norm = pred_unc_map - mu_pred\n",
    "    prod = np.sum(np.multiply(gt_norm, pred_norm))\n",
    "    ncc = (1 / (np.size(gt_unc_map) * sigma_gt * sigma_pred)) * prod\n",
    "    return ncc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AURC et EAURC\n",
    "\"\"\"\n",
    "risk=array avec erreurs/loss pour chaque image\n",
    "confids=confiance dans la préd\n",
    "\"\"\"\n",
    "def rc_curve_stats(\n",
    "    risks: np.array, confids: np.array\n",
    ") -> tuple[list[float], list[float], list[float]]:\n",
    "    coverages = []\n",
    "    selective_risks = []\n",
    "    assert (\n",
    "        len(risks.shape) == 1 and len(confids.shape) == 1 and len(risks) == len(confids)\n",
    "    )\n",
    "\n",
    "    n_samples = len(risks)\n",
    "    idx_sorted = np.argsort(confids)\n",
    "\n",
    "    coverage = n_samples\n",
    "    error_sum = sum(risks[idx_sorted])\n",
    "\n",
    "    coverages.append(coverage / n_samples)\n",
    "    selective_risks.append(error_sum / n_samples)\n",
    "\n",
    "    weights = []\n",
    "\n",
    "    tmp_weight = 0\n",
    "    for i in range(0, len(idx_sorted) - 1):\n",
    "        coverage = coverage - 1\n",
    "        error_sum = error_sum - risks[idx_sorted[i]]\n",
    "        tmp_weight += 1\n",
    "        if i == 0 or confids[idx_sorted[i]] != confids[idx_sorted[i - 1]]:\n",
    "            coverages.append(coverage / n_samples)\n",
    "            selective_risks.append(error_sum / (n_samples - 1 - i))\n",
    "            weights.append(tmp_weight / n_samples)\n",
    "            tmp_weight = 0\n",
    "\n",
    "def aurc(risks: np.array, confids: np.array):\n",
    "    _, risks, weights = rc_curve_stats(risks, confids)\n",
    "    return sum(\n",
    "        [(risks[i] + risks[i + 1]) * 0.5 * weights[i] for i in range(len(weights))]\n",
    "    )\n",
    "\n",
    "def eaurc(risks: np.array, confids: np.array):\n",
    "    \"\"\"Compute normalized AURC, i.e. subtract AURC of optimal CSF (given fixed risks).\"\"\"\n",
    "    n = len(risks)\n",
    "    # optimal confidence sorts risk. Asencding here because we start from coverage 1/n\n",
    "    selective_risks = np.sort(risks).cumsum() / np.arange(1, n + 1)\n",
    "    aurc_opt = selective_risks.sum() / n\n",
    "    return aurc(risks, confids) - aurc_opt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

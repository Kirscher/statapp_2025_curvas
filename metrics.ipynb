{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.calibration import _sigmoid_calibration as calib\n",
    "from sklearn import utils as sk_utils\n",
    "from sklearn import preprocessing as sk_preprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "from evaluation.experiment_dataloader import ExperimentDataloader\n",
    "\n",
    "\n",
    "def platt_scale_params(val_exp_dataloader: ExperimentDataloader, ignore_value=None):\n",
    "    ps_params_dict = {}\n",
    "    for unc_type in val_exp_dataloader.exp_version.unc_types:\n",
    "        ps_params_dict[unc_type] = {\"a\": [], \"b\": []}\n",
    "        for image_id in tqdm(val_exp_dataloader.image_ids):\n",
    "            reference_segs = val_exp_dataloader.get_reference_segs(image_id)\n",
    "            pred_seg = val_exp_dataloader.get_mean_pred_seg(image_id)\n",
    "            unc_map = val_exp_dataloader.get_unc_map(image_id, unc_type)\n",
    "            # 2d unc map is loaded in shape (W, H)\n",
    "            if pred_seg.shape != unc_map.shape:\n",
    "                unc_map = np.swapaxes(unc_map, 0, 1)\n",
    "            pred_seg = np.repeat(pred_seg[np.newaxis, :], reference_segs.shape[0], 0)\n",
    "            unc_map = np.repeat(unc_map[np.newaxis, :], reference_segs.shape[0], 0)\n",
    "            rater_correct = (reference_segs == pred_seg).astype(int)\n",
    "            if ignore_value is not None:\n",
    "                ignore_mask = reference_segs != ignore_value\n",
    "                a, b = calib(-unc_map[ignore_mask], rater_correct[ignore_mask])\n",
    "            else:\n",
    "                a, b = calib(-unc_map.flatten(), np.array(rater_correct).flatten())\n",
    "            ps_params_dict[unc_type][\"a\"].append(a)\n",
    "            ps_params_dict[unc_type][\"b\"].append(b)\n",
    "        ps_params_dict[unc_type][\"a\"] = np.mean(np.array(ps_params_dict[unc_type][\"a\"]))\n",
    "        ps_params_dict[unc_type][\"b\"] = np.mean(np.array(ps_params_dict[unc_type][\"b\"]))\n",
    "    with open(\n",
    "        val_exp_dataloader.exp_version.exp_path / \"platt_scale_params.json\", \"w\"\n",
    "    ) as f:\n",
    "        json.dump(ps_params_dict, f, indent=2)\n",
    "\n",
    "\n",
    "def platt_scale_confid(uncalib_confid, platt_scale_file, uncertainty):\n",
    "    with open(platt_scale_file) as f:\n",
    "        params_dict = json.load(f)\n",
    "    params = params_dict[uncertainty]\n",
    "    return 1 / (1 + np.exp(uncalib_confid * params[\"a\"] + params[\"b\"]))\n",
    "\n",
    "\n",
    "def calib_stats(correct, calib_confids):\n",
    "    # calib_confids = np.clip(self.confids, 0, 1)\n",
    "    n_bins = 20\n",
    "    y_true = sk_utils.column_or_1d(correct)\n",
    "    y_prob = sk_utils.column_or_1d(calib_confids)\n",
    "\n",
    "    if y_prob.min() < 0 or y_prob.max() > 1:\n",
    "        raise ValueError(\n",
    "            \"y_prob has values outside [0, 1] and normalize is \" \"set to False.\"\n",
    "        )\n",
    "\n",
    "    labels = np.unique(y_true)\n",
    "    if len(labels) > 2:\n",
    "        raise ValueError(\n",
    "            \"Only binary classification is supported. \" f\"Provided labels {labels}.\"\n",
    "        )\n",
    "    y_true = sk_preprocess.label_binarize(y_true, classes=labels)[:, 0]\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0 + 1e-8, n_bins + 1)\n",
    "\n",
    "    binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "    bin_sums = np.bincount(binids, weights=y_prob, minlength=len(bins))\n",
    "    bin_true = np.bincount(binids, weights=y_true, minlength=len(bins))\n",
    "    bin_total = np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "    nonzero = bin_total != 0\n",
    "    num_nonzero = len(nonzero[nonzero == True])\n",
    "    prob_true = bin_true[nonzero] / bin_total[nonzero]\n",
    "    prob_pred = bin_sums[nonzero] / bin_total[nonzero]\n",
    "    prob_total = bin_total[nonzero] / bin_total.sum()\n",
    "\n",
    "    bin_discrepancies = np.abs(prob_true - prob_pred)\n",
    "    return bin_discrepancies, prob_total, num_nonzero\n",
    "\n",
    "\n",
    "def calc_ace(correct, calib_confids):\n",
    "    bin_discrepancies, _, num_nonzero = calib_stats(correct, calib_confids)\n",
    "    return (1 / num_nonzero) * np.sum(bin_discrepancies)\n",
    "\n",
    "\n",
    "def calibration_error(exp_dataloader: ExperimentDataloader, ignore_value=None):\n",
    "    calib_dict = {}\n",
    "    calib_dict[\"mean\"] = {}\n",
    "    for unc_type in exp_dataloader.exp_version.unc_types:\n",
    "        aces_unc = []\n",
    "        for image_id in tqdm(exp_dataloader.image_ids):\n",
    "            if image_id not in calib_dict.keys():\n",
    "                calib_dict[image_id] = {}\n",
    "            reference_segs = exp_dataloader.get_reference_segs(image_id)\n",
    "            pred_seg = exp_dataloader.get_mean_pred_seg(image_id)\n",
    "            unc_map = exp_dataloader.get_unc_map(image_id, unc_type)\n",
    "            # 2d unc map is loaded in shape (W, H)\n",
    "            if pred_seg.shape != unc_map.shape:\n",
    "                unc_map = np.swapaxes(unc_map, 0, 1)\n",
    "            pred_seg = np.repeat(pred_seg[np.newaxis, :], reference_segs.shape[0], 0)\n",
    "            unc_map = np.repeat(unc_map[np.newaxis, :], reference_segs.shape[0], 0)\n",
    "            rater_correct = (reference_segs == pred_seg).astype(int)\n",
    "            platt_scale_file = (\n",
    "                exp_dataloader.exp_version.exp_path / \"platt_scale_params.json\"\n",
    "            )\n",
    "            if ignore_value is not None:\n",
    "                ignore_mask = reference_segs != ignore_value\n",
    "                unc_map = platt_scale_confid(\n",
    "                    -unc_map[ignore_mask],\n",
    "                    platt_scale_file=platt_scale_file,\n",
    "                    uncertainty=unc_type,\n",
    "                )\n",
    "                ace = calc_ace(rater_correct[ignore_mask], unc_map)\n",
    "                calib_dict[image_id][unc_type] = {\"metrics\": {\"ace\": ace}}\n",
    "                aces_unc.append(ace)\n",
    "            else:\n",
    "                unc_map = platt_scale_confid(\n",
    "                    -unc_map.flatten(),\n",
    "                    platt_scale_file=platt_scale_file,\n",
    "                    uncertainty=unc_type,\n",
    "                )\n",
    "                ace = calc_ace(rater_correct.flatten(), unc_map)\n",
    "                calib_dict[image_id][unc_type] = {\"metrics\": {\"ace\": ace}}\n",
    "                aces_unc.append(ace)\n",
    "        calib_dict[\"mean\"][unc_type] = {\"metrics\": {\"ace\": np.mean(np.array(aces_unc))}}\n",
    "    save_path = exp_dataloader.dataset_path / \"calibration.json\"\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(calib_dict, f, indent=2)\n",
    "\n",
    "\n",
    "def main(exp_dataloader: ExperimentDataloader, ignore_value=None):\n",
    "    platt_scale_params_file = (\n",
    "        exp_dataloader.exp_version.exp_path / \"platt_scale_params.json\"\n",
    "    )\n",
    "    # replace by checking whether platt scale params file exists\n",
    "    if not os.path.isfile(platt_scale_params_file):\n",
    "        val_exp_dataloader = ExperimentDataloader(exp_dataloader.exp_version, \"val\")\n",
    "        platt_scale_params(val_exp_dataloader, ignore_value=ignore_value)\n",
    "    calibration_error(exp_dataloader, ignore_value=ignore_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NCC\n",
    "def compute_ncc(gt_unc_map: np.array, pred_unc_map: np.array):\n",
    "    \"\"\"\n",
    "    Compute the normalized cross correlation between a ground truth uncertainty and a predicted uncertainty map,\n",
    "    to determine how similar the maps are.\n",
    "    :param gt_unc_map: the ground truth uncertainty map based on the rater variability\n",
    "    :param pred_unc_map: the predicted uncertainty map\n",
    "    :return: float: the normalized cross correlation between gt and predicted uncertainty map\n",
    "    \"\"\"\n",
    "    mu_gt = np.mean(gt_unc_map)\n",
    "    mu_pred = np.mean(pred_unc_map)\n",
    "    sigma_gt = np.std(gt_unc_map, ddof=1)\n",
    "    sigma_pred = np.std(pred_unc_map, ddof=1)\n",
    "    gt_norm = gt_unc_map - mu_gt\n",
    "    pred_norm = pred_unc_map - mu_pred\n",
    "    prod = np.sum(np.multiply(gt_norm, pred_norm))\n",
    "    ncc = (1 / (np.size(gt_unc_map) * sigma_gt * sigma_pred)) * prod\n",
    "    return ncc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AURC et EAURC\n",
    "\"\"\"\n",
    "risk=array avec erreurs/loss pour chaque image\n",
    "confids=confiance dans la prÃ©d\n",
    "\"\"\"\n",
    "def rc_curve_stats(\n",
    "    risks: np.array, confids: np.array\n",
    ") -> tuple[list[float], list[float], list[float]]:\n",
    "    coverages = []\n",
    "    selective_risks = []\n",
    "    assert (\n",
    "        len(risks.shape) == 1 and len(confids.shape) == 1 and len(risks) == len(confids)\n",
    "    )\n",
    "\n",
    "    n_samples = len(risks)\n",
    "    idx_sorted = np.argsort(confids)\n",
    "\n",
    "    coverage = n_samples\n",
    "    error_sum = sum(risks[idx_sorted])\n",
    "\n",
    "    coverages.append(coverage / n_samples)\n",
    "    selective_risks.append(error_sum / n_samples)\n",
    "\n",
    "    weights = []\n",
    "\n",
    "    tmp_weight = 0\n",
    "    for i in range(0, len(idx_sorted) - 1):\n",
    "        coverage = coverage - 1\n",
    "        error_sum = error_sum - risks[idx_sorted[i]]\n",
    "        tmp_weight += 1\n",
    "        if i == 0 or confids[idx_sorted[i]] != confids[idx_sorted[i - 1]]:\n",
    "            coverages.append(coverage / n_samples)\n",
    "            selective_risks.append(error_sum / (n_samples - 1 - i))\n",
    "            weights.append(tmp_weight / n_samples)\n",
    "            tmp_weight = 0\n",
    "\n",
    "def aurc(risks: np.array, confids: np.array):\n",
    "    _, risks, weights = rc_curve_stats(risks, confids)\n",
    "    return sum(\n",
    "        [(risks[i] + risks[i + 1]) * 0.5 * weights[i] for i in range(len(weights))]\n",
    "    )\n",
    "\n",
    "def eaurc(risks: np.array, confids: np.array):\n",
    "    \"\"\"Compute normalized AURC, i.e. subtract AURC of optimal CSF (given fixed risks).\"\"\"\n",
    "    n = len(risks)\n",
    "    # optimal confidence sorts risk. Asencding here because we start from coverage 1/n\n",
    "    selective_risks = np.sort(risks).cumsum() / np.arange(1, n + 1)\n",
    "    aurc_opt = selective_risks.sum() / n\n",
    "    return aurc(risks, confids) - aurc_opt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

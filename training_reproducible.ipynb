{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b00aecb",
   "metadata": {},
   "source": [
    "# Reproducible Training of nnU-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdfa961",
   "metadata": {},
   "source": [
    "Here is the code to reproduce the training of nnU-Net according to the dataset you want and initialization (not yet available).\n",
    "This Jupyter Notebook is available on the branch test_leo (``git checkout test_leo``).\n",
    "\n",
    "Very Important: Before doing anything on this notebook, you should open a service Onyxia entitled \"Vscode-pytorch-gpu\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9b536",
   "metadata": {},
   "source": [
    "**What is missing?**\n",
    "- Early stopping (heuristic: 80 epochs which lasts ~5h)\n",
    "- Different initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f7f49",
   "metadata": {},
   "source": [
    "## 1. Requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db46d5",
   "metadata": {},
   "source": [
    "Python libraries required to run training and handle document downloading / uploading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e005d3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nnunetv2 in /usr/local/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: s3fs in /usr/local/lib/python3.12/site-packages (2025.3.2)\n",
      "Requirement already satisfied: torch>=2.1.2 in /usr/local/lib/python3.12/site-packages (from nnunetv2) (2.7.0)\n",
      "Requirement already satisfied: acvl-utils<0.3,>=0.2.3 in /usr/local/lib/python3.12/site-packages (from nnunetv2) (0.2.5)\n",
      "Requirement already satisfied: dynamic-network-architectures<0.4,>=0.3.1 in /usr/local/lib/python3.12/site-packages (from nnunetv2) (0.3.1)\n",
      "Requirement already satisfied: dicom2nifti in /usr/local/lib/python3.12/site-packages (from nnunetv2) (2.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/site-packages (from nnunetv2) (1.15.2)\n",
      "Requirement already satisfied: batchgenerators>=0.25.1 in /usr/local/lib/python3.12/site-packages (from nnunetv2) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/site-packages (from nnunetv2) (2.2.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/site-packages (from nnunetv2) (1.6.1)\n",
      "Requirement already satisfied: scikit-image>=0.19.3 in /usr/local/lib/python3.12/site-packages (from nnunetv2) (0.25.2)\n",
      "Requirement already satisfied: SimpleITK>=2.2.1 in /usr/local/lib/python3.12/site-packages (from nnunetv2) (2.5.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/site-packages (from nnunetv2) (2.2.3)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/site-packages (from nnunetv2) (0.20.3)\n",
      "Requirement already satisfied: tifffile in /usr/local/lib/python3.12/site-packages (from nnunetv2) (2025.3.30)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from nnunetv2) (2.32.3)\n",
      "Requirement already satisfied: nibabel in /usr/local/lib/python3.12/site-packages (from nnunetv2) (5.3.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/site-packages (from nnunetv2) (3.10.1)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/site-packages (from nnunetv2) (0.13.2)\n",
      "Requirement already satisfied: imagecodecs in /usr/local/lib/python3.12/site-packages (from nnunetv2) (2025.3.30)\n",
      "Requirement already satisfied: yacs in /usr/local/lib/python3.12/site-packages (from nnunetv2) (0.1.8)\n",
      "Requirement already satisfied: batchgeneratorsv2>=0.2 in /usr/local/lib/python3.12/site-packages (from nnunetv2) (0.2.3)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/site-packages (from nnunetv2) (0.8.1)\n",
      "Requirement already satisfied: blosc2>=3.0.0b1 in /usr/local/lib/python3.12/site-packages (from nnunetv2) (3.3.2)\n",
      "Requirement already satisfied: connected-components-3d in /usr/local/lib/python3.12/site-packages (from acvl-utils<0.3,>=0.2.3->nnunetv2) (3.23.0)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.12/site-packages (from s3fs) (2.21.1)\n",
      "Requirement already satisfied: fsspec==2025.3.2.* in /usr/local/lib/python3.12/site-packages (from s3fs) (2025.3.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/site-packages (from s3fs) (3.11.18)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.37.2,>=1.37.0 in /usr/local/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.37.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0.post0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (6.4.3)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.6.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.20.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/site-packages (from botocore<1.37.2,>=1.37.0->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.10)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/site-packages (from batchgenerators>=0.25.1->nnunetv2) (11.2.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.12/site-packages (from batchgenerators>=0.25.1->nnunetv2) (1.0.0)\n",
      "Requirement already satisfied: unittest2 in /usr/local/lib/python3.12/site-packages (from batchgenerators>=0.25.1->nnunetv2) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.12/site-packages (from batchgenerators>=0.25.1->nnunetv2) (3.6.0)\n",
      "Requirement already satisfied: fft-conv-pytorch in /usr/local/lib/python3.12/site-packages (from batchgeneratorsv2>=0.2->nnunetv2) (1.2.0)\n",
      "Requirement already satisfied: ndindex in /usr/local/lib/python3.12/site-packages (from blosc2>=3.0.0b1->nnunetv2) (1.9.2)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/site-packages (from blosc2>=3.0.0b1->nnunetv2) (1.1.0)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/site-packages (from blosc2>=3.0.0b1->nnunetv2) (4.3.7)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/site-packages (from blosc2>=3.0.0b1->nnunetv2) (2.10.2)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/site-packages (from blosc2>=3.0.0b1->nnunetv2) (9.0.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/site-packages (from scikit-image>=0.19.3->nnunetv2) (3.4.2)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/site-packages (from scikit-image>=0.19.3->nnunetv2) (2.37.0)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/site-packages (from scikit-image>=0.19.3->nnunetv2) (25.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/site-packages (from scikit-image>=0.19.3->nnunetv2) (0.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (80.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.1.2->nnunetv2) (1.3.0)\n",
      "Requirement already satisfied: pydicom>=3.0.0 in /usr/local/lib/python3.12/site-packages (from dicom2nifti->nnunetv2) (3.0.1)\n",
      "Requirement already satisfied: python-gdcm in /usr/local/lib/python3.12/site-packages (from dicom2nifti->nnunetv2) (3.0.24.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch>=2.1.2->nnunetv2) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/site-packages (from matplotlib->nnunetv2) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/site-packages (from matplotlib->nnunetv2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/site-packages (from matplotlib->nnunetv2) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib->nnunetv2) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib->nnunetv2) (3.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas->nnunetv2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas->nnunetv2) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->nnunetv2) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests->nnunetv2) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn->nnunetv2) (1.4.2)\n",
      "Collecting argparse (from unittest2->batchgenerators>=0.25.1->nnunetv2)\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: traceback2 in /usr/local/lib/python3.12/site-packages (from unittest2->batchgenerators>=0.25.1->nnunetv2) (1.4.0)\n",
      "Requirement already satisfied: linecache2 in /usr/local/lib/python3.12/site-packages (from traceback2->unittest2->batchgenerators>=0.25.1->nnunetv2) (1.0.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/site-packages (from yacs->nnunetv2) (6.0.2)\n",
      "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nnunetv2 tqdm s3fs\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import s3fs\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff484b16",
   "metadata": {},
   "source": [
    "Before training the models, you need to enter your credentials. Example: email --> blabla.blabla@ensae.fr, name --> username Onyxia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc69f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git configured with email : leo.leroy@ensae.fr and username : leoacpr\n"
     ]
    }
   ],
   "source": [
    "email = input(\"Enter your email ENSAE: \")\n",
    "name = input(\"Enter your username Onyxia: \")\n",
    "\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", email])\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", name])\n",
    "\n",
    "print(f\"Git configured with email : {email} and username : {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20327ed2",
   "metadata": {},
   "source": [
    "Now, you must enter your S3 private keys. They are available on Onyxia > Account > Connexion au stockage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef67856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS keys configured as environment variables.\n"
     ]
    }
   ],
   "source": [
    "aws_access_key_id = input(\"Enter your AWS_ACCESS_KEY_ID: \")\n",
    "aws_secret_access_key = input(\"Enter your AWS_SECRET_ACCESS_KEY: \")\n",
    "aws_session_token = input(\"Enter your AWS_SESSION_TOKEN: \")\n",
    "\n",
    "# Environment variables\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = aws_access_key_id\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = aws_secret_access_key\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = aws_session_token\n",
    "\n",
    "print(\"AWS keys configured as environment variables.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3897125b",
   "metadata": {},
   "source": [
    "## 2. Downloading files from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfde054",
   "metadata": {},
   "source": [
    "The datasets are stored on the S3 service provided by Onyxia. The are available on the path   ``projet-statapp-segmedic/diffusion``. You need to download them locally by running the code below. Estimated time: 4 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b74a21c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion to  MinIO S3 Onyxia\n",
    "s3 = s3fs.S3FileSystem(\n",
    "    client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},\n",
    "    key=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    secret=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    token=os.getenv(\"AWS_SESSION_TOKEN\")\n",
    ")\n",
    "#print(len(s3.ls(\"projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_raw/Dataset001_Annot1/labelsTr\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9254a8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Téléchargement du dossier nnUNet_raw...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fichiers dans nnUNet_raw: 100%|██████████| 133/133 [02:07<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Téléchargement du dossier nnUNet_preprocessed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fichiers dans nnUNet_preprocessed: 100%|██████████| 253/253 [00:00<00:00, 33779.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Téléchargement du dossier nnUNet_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fichiers dans nnUNet_results: 100%|██████████| 45/45 [00:00<00:00, 27310.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration finished. Environment variables created:\n",
      "nnUNet_raw=/tmp/nnunet/nnUNet_raw\n",
      "nnUNet_preprocessed=/tmp/nnunet/nnUNet_preprocessed\n",
      "nnUNet_results=/tmp/nnunet/nnUNet_results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def download_s3_folder():\n",
    "    \n",
    "    # Defining paths\n",
    "    base_local_path = Path('/tmp/nnunet')\n",
    "    s3_base_path = \"projet-statapp-segmedic/diffusion/nnunet_dataset\"\n",
    "    folders = ['nnUNet_raw', 'nnUNet_preprocessed', 'nnUNet_results']\n",
    "    \n",
    "    # Creating local folders\n",
    "    for folder in folders:\n",
    "        local_folder = base_local_path / folder\n",
    "        local_folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        s3_path = f\"{s3_base_path}/{folder}\"\n",
    "        print(f\"\\nTéléchargement du dossier {folder}...\")\n",
    "        \n",
    "        # Recursive list of all files from S3\n",
    "        try:\n",
    "            files = s3.find(s3_path)\n",
    "            \n",
    "            # Progression bar (very nice!)\n",
    "            with tqdm(total=len(files), desc=f\"Fichiers dans {folder}\") as pbar:\n",
    "                for file_path in files:\n",
    "                    relative_path = file_path.replace(s3_path, '').lstrip('/')\n",
    "                    local_file_path = local_folder / relative_path\n",
    "                    \n",
    "                    # Creating local files if needed\n",
    "                    local_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    \n",
    "                    # Dowloading files\n",
    "                    if not local_file_path.exists():\n",
    "                        try:\n",
    "                            s3.get(file_path, str(local_file_path))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error while downloading {file_path}: {e}\")\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while reading {s3_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        #ERROR CORRECTED: the nnU-Net dataset naming convention requires 4 digit for image case file, not 3. \n",
    "        for string in ['1', '2', '3']:\n",
    "            images = Path(f\"/tmp/nnunet/nnUNet_raw/Dataset00{string}_Annot{string}/imagesTr\")\n",
    "            for f in images.glob(\"*_000.nii.gz\"):\n",
    "                f.rename(f.with_name(f.name.replace(\"_000.nii.gz\", \"_0000.nii.gz\")))\n",
    "    \n",
    "    # Creating global variables for paths, needed for nnU-Net training. \n",
    "    env_vars = {\n",
    "        'nnUNet_raw': str(base_local_path / 'nnUNet_raw'),\n",
    "        'nnUNet_preprocessed': str(base_local_path / 'nnUNet_preprocessed'),\n",
    "        'nnUNet_results': str(base_local_path / 'nnUNet_results')\n",
    "    }\n",
    "    \n",
    "    for var_name, path in env_vars.items():\n",
    "        os.environ[var_name] = path\n",
    "    \n",
    "    # Adding to .bashrc\n",
    "    with open(os.path.expanduser('~/.bashrc'), 'a') as f:\n",
    "        f.write('\\n# nnUNet paths\\n')\n",
    "        for var_name, path in env_vars.items():\n",
    "            f.write(f'export {var_name}=\"{path}\"\\n')\n",
    "    \n",
    "    print(\"\\nConfiguration finished. Environment variables created:\")\n",
    "    for var_name, path in env_vars.items():\n",
    "        print(f\"{var_name}={path}\")\n",
    "\n",
    "    #To apply changes:\n",
    "    !source ~/.bashrc\n",
    "\n",
    "download_s3_folder()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd7a23",
   "metadata": {},
   "source": [
    "Verify if the downloading has been done successfully by running the following line. \n",
    "\n",
    "Expected output: _dataset_fingerprint.json gt_segmentations nnUNetPlans.json dataset.json nnUNetPlans_3d_fullres splits_final.json_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d07893d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_fingerprint.json  gt_segmentations\t  nnUNetPlans.json\n",
      "dataset.json\t\t  nnUNetPlans_3d_fullres  splits_final.json\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/nnunet/nnUNet_preprocessed/Dataset002_Annot2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986f4c7",
   "metadata": {},
   "source": [
    "If you wish to preprocess and verify the datasets integrity, copy-paste and run the following lines. **BE CAREFUL:** this might make Onyxia crash if you do not increase the CPU and RAM ressources! It also takes more than 20 min per line.The lines have already been run before. You normally do not need to run them. That is why the lines are not in a code cell.\n",
    "\n",
    "``!nnUNetv2_plan_and_preprocess -h``\n",
    "\n",
    "``!nnUNetv2_plan_and_preprocess -d Dataset001_Annot1 -c 3d_fullres --verify_dataset_integrity -np 2 -npfp 2``\n",
    "\n",
    "``!nnUNetv2_plan_and_preprocess -d Dataset002_Annot2 -c 3d_fullres --verify_dataset_integrity -np 2 -npfp 2``\n",
    "\n",
    "``!nnUNetv2_plan_and_preprocess -d Dataset003_Annot3 -c 3d_fullres --verify_dataset_integrity -np 2 -npfp 2``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b1466",
   "metadata": {},
   "source": [
    "(**Optional**) If you wish to upload all the documents stored locally, you can run the following code. Select one folder among ``nnUNet_preprocessed`` or ``nnUNet_results`` (you normally do not need to upload files from nnUNet_raw). Estimated time: between 10s and 1min10s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b78d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(folder):\n",
    "    # Dossier local et distant\n",
    "    local_folder = Path(f'/tmp/nnunet/{folder}')\n",
    "    s3_folder = f\"projet-statapp-segmedic/diffusion/nnunet_dataset/{folder}\"\n",
    "    \n",
    "    # Lister tous les fichiers à uploader\n",
    "    files = list(local_folder.rglob(\"*\"))\n",
    "    \n",
    "    print(f\"\\nUploading {folder} to {s3_folder}...\")\n",
    "    with tqdm(total=len(files), desc=f\"Upload {folder}\") as pbar:\n",
    "        for file_path in files:\n",
    "            if file_path.is_file():\n",
    "                relative_path = file_path.relative_to(local_folder)\n",
    "                s3_path = f\"{s3_folder}/{relative_path.as_posix()}\"\n",
    "                try:\n",
    "                    s3.put(str(file_path), s3_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lors de l'upload de {file_path} → {s3_path}: {e}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "#upload_to_s3(input(\"Enter nnUNet_preprocessed or nnUNet_results\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f83170",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0b2f6",
   "metadata": {},
   "source": [
    "Training must be jointly done with file uploading: The training creates many documents to save progress. These documents are stored locally, but we need them on S3. Given that epochs take usually about 200s, I decided to set the time interval of uploading to 200s.\n",
    "\n",
    "Decide on which dataset (i.e. which set of annotations) you want to use: ``Dataset001_Annot1``, ``Dataset002_Annot2``, ``Dataset003_Annot3``. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebb6190",
   "metadata": {},
   "source": [
    "**CAREFUL**: The project isn't entirely done. For the moment, there is no early stopping. You should continuoulsy check if Onyxia hasn't crashed during the training (normally it shouldn't happen) and stop about 80 epochs. If you wish to resume training, you can enter this: ``nnUNetv2_train <dataset> 3d_fullres all --npz --c`` but it will only resume from a multiple of 50 epochs (nnU-Net automatically saves its results every 50 epochs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208da390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Uploader] Starting S3 sync thread.\n",
      "\n",
      "Uploading nnUNet_results to projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_results...\n",
      "[Trainer] Launching nnUNet training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results:  48%|████▊     | 30/62 [00:05<00:03,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results:  53%|█████▎    | 33/62 [00:08<00:12,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-01 21:06:07.090267: Using torch.compile...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results:  66%|██████▌   | 41/62 [00:11<00:09,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-01 21:06:09.558565: do_dummy_2d_data_aug: False\n",
      "using pin_memory on device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results:  73%|███████▎  | 45/62 [00:14<00:08,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pin_memory on device 0\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [192, 112, 112], 'median_image_size_in_voxels': [943.0, 512.0, 512.0], 'spacing': [1.0, 0.9626015722751617, 0.9626015722751617], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 1, 1]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_Annot1', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 0.9626015722751617, 0.9626015722751617], 'original_median_shape_after_transp': [956, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 901.9999389648438, 'mean': 116.67008209228516, 'median': 113.99999237060547, 'min': -1036.0001220703125, 'percentile_00_5': -34.99999237060547, 'percentile_99_5': 255.0, 'std': 43.83061599731445}}} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results:  76%|███████▌  | 47/62 [00:14<00:05,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-01 21:06:13.151583: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-05-01 21:06:13.170194: \n",
      "2025-05-01 21:06:13.171853: Epoch 50\n",
      "2025-05-01 21:06:13.172388: Current learning rate: 0.00955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results: 100%|██████████| 62/62 [00:20<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload done\n",
      "\n",
      "Uploading nnUNet_results to projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results: 100%|██████████| 63/63 [00:21<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload done\n",
      "2025-05-01 21:11:11.027475: train_loss -0.6404\n",
      "2025-05-01 21:11:11.028437: val_loss -0.7084\n",
      "2025-05-01 21:11:11.028638: Pseudo dice [np.float32(0.8433), np.float32(0.9555), np.float32(0.9364)]\n",
      "2025-05-01 21:11:11.029136: Epoch time: 297.86 s\n",
      "2025-05-01 21:11:11.029487: Yayy! New best EMA pseudo Dice: 0.8966000080108643\n",
      "2025-05-01 21:11:13.900142: \n",
      "2025-05-01 21:11:13.900580: Epoch 51\n",
      "2025-05-01 21:11:13.900786: Current learning rate: 0.00954\n"
     ]
    }
   ],
   "source": [
    "# Code to train and upload nnU-Net\n",
    "\n",
    "dataset=input(\"Enter one among: Dataset001_Annot1, Dataset002_Annot2, Dataset003_Annot3\")\n",
    "\n",
    "# Upload function with time interval \n",
    "# IDEA: upload as soon as the content of temp/results changes\n",
    "def sync_results_to_s3():\n",
    "    print(\"[Uploader] Starting S3 sync thread.\")\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        upload_to_s3('nnUNet_results')\n",
    "        print(\"upload done\")\n",
    "\n",
    "        time.sleep(300)\n",
    "\n",
    "\n",
    "# Training function\n",
    "def run_training():\n",
    "    print(\"[Trainer] Launching nnUNet training...\")\n",
    "    command = [\n",
    "        \"nnUNetv2_train\",\n",
    "        f\"{dataset}\",  # Dataset ID\n",
    "        \"3d_fullres\",  # Plan\n",
    "        \"all\",  # Fold            \n",
    "        \"--npz\",\n",
    "        \"--c\"\n",
    "    ]\n",
    "    subprocess.run(command)\n",
    "    print(\"[Trainer] Training complete.\")\n",
    "\n",
    "\n",
    "# Threads\n",
    "uploader_thread = threading.Thread(target=sync_results_to_s3, daemon=True)\n",
    "trainer_thread = threading.Thread(target=run_training)\n",
    "\n",
    "uploader_thread.start()\n",
    "trainer_thread.start()\n",
    "\n",
    "trainer_thread.join()\n",
    "print(\"[Main] All done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b97d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33596f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

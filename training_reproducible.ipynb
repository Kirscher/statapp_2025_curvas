{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b00aecb",
   "metadata": {},
   "source": [
    "# Reproducible Training of nnU-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdfa961",
   "metadata": {},
   "source": [
    "Here is the code to reproduce the training of nnU-Net according to the dataset you want and initialization (not yet available).\n",
    "This Jupyter Notebook is available on the branch test_leo (``git checkout test_leo``).\n",
    "\n",
    "Very Important: Before doing anything on this notebook, you should open a service Onyxia entitled \"Vscode-pytorch-gpu\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9b536",
   "metadata": {},
   "source": [
    "**What is missing?**\n",
    "- Early stopping (heuristic: 80 epochs which lasts ~5h)\n",
    "- Different initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f7f49",
   "metadata": {},
   "source": [
    "## 1. Requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db46d5",
   "metadata": {},
   "source": [
    "Python libraries required to run training and handle document downloading / uploading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e005d3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nnunetv2\n",
      "  Downloading nnunetv2-2.6.0.tar.gz (206 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: s3fs in /usr/local/lib/python3.12/site-packages (2025.3.2)\n",
      "Requirement already satisfied: torch>=2.1.2 in /usr/local/lib/python3.12/site-packages (from nnunetv2) (2.7.0)\n",
      "Collecting acvl-utils<0.3,>=0.2.3 (from nnunetv2)\n",
      "  Downloading acvl_utils-0.2.5.tar.gz (29 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting dynamic-network-architectures<0.4,>=0.3.1 (from nnunetv2)\n",
      "  Downloading dynamic_network_architectures-0.3.1.tar.gz (20 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting dicom2nifti (from nnunetv2)\n",
      "  Downloading dicom2nifti-2.6.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting scipy (from nnunetv2)\n",
      "  Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting batchgenerators>=0.25.1 (from nnunetv2)\n",
      "  Downloading batchgenerators-0.25.1.tar.gz (76 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/site-packages (from nnunetv2) (2.2.5)\n",
      "Collecting scikit-learn (from nnunetv2)\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting scikit-image>=0.19.3 (from nnunetv2)\n",
      "  Downloading scikit_image-0.25.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting SimpleITK>=2.2.1 (from nnunetv2)\n",
      "  Downloading simpleitk-2.5.0-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/site-packages (from nnunetv2) (2.2.3)\n",
      "Collecting graphviz (from nnunetv2)\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tifffile (from nnunetv2)\n",
      "  Downloading tifffile-2025.3.30-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from nnunetv2) (2.32.3)\n",
      "Collecting nibabel (from nnunetv2)\n",
      "  Downloading nibabel-5.3.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting matplotlib (from nnunetv2)\n",
      "  Downloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting seaborn (from nnunetv2)\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting imagecodecs (from nnunetv2)\n",
      "  Downloading imagecodecs-2025.3.30-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting yacs (from nnunetv2)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting batchgeneratorsv2>=0.2 (from nnunetv2)\n",
      "  Downloading batchgeneratorsv2-0.2.3.tar.gz (35 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting einops (from nnunetv2)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting blosc2>=3.0.0b1 (from nnunetv2)\n",
      "  Downloading blosc2-3.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting connected-components-3d (from acvl-utils<0.3,>=0.2.3->nnunetv2)\n",
      "  Downloading connected_components_3d-3.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.12/site-packages (from s3fs) (2.21.1)\n",
      "Requirement already satisfied: fsspec==2025.3.2.* in /usr/local/lib/python3.12/site-packages (from s3fs) (2025.3.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/site-packages (from s3fs) (3.11.18)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.37.2,>=1.37.0 in /usr/local/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.37.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0.post0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (6.4.3)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.6.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.20.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/site-packages (from botocore<1.37.2,>=1.37.0->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.10)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/site-packages (from batchgenerators>=0.25.1->nnunetv2) (11.2.1)\n",
      "Collecting future (from batchgenerators>=0.25.1->nnunetv2)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting unittest2 (from batchgenerators>=0.25.1->nnunetv2)\n",
      "  Downloading unittest2-1.1.0-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Collecting threadpoolctl (from batchgenerators>=0.25.1->nnunetv2)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fft-conv-pytorch (from batchgeneratorsv2>=0.2->nnunetv2)\n",
      "  Downloading fft_conv_pytorch-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting ndindex (from blosc2>=3.0.0b1->nnunetv2)\n",
      "  Downloading ndindex-1.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting msgpack (from blosc2>=3.0.0b1->nnunetv2)\n",
      "  Downloading msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/site-packages (from blosc2>=3.0.0b1->nnunetv2) (4.3.7)\n",
      "Collecting numexpr (from blosc2>=3.0.0b1->nnunetv2)\n",
      "  Downloading numexpr-2.10.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting py-cpuinfo (from blosc2>=3.0.0b1->nnunetv2)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/site-packages (from scikit-image>=0.19.3->nnunetv2) (3.4.2)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image>=0.19.3->nnunetv2)\n",
      "  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/site-packages (from scikit-image>=0.19.3->nnunetv2) (25.0)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image>=0.19.3->nnunetv2)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (80.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.12/site-packages (from torch>=2.1.2->nnunetv2) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.1.2->nnunetv2) (1.3.0)\n",
      "Collecting pydicom>=3.0.0 (from dicom2nifti->nnunetv2)\n",
      "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting python-gdcm (from dicom2nifti->nnunetv2)\n",
      "  Downloading python_gdcm-3.0.24.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch>=2.1.2->nnunetv2) (3.0.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->nnunetv2)\n",
      "  Downloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->nnunetv2)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->nnunetv2)\n",
      "  Downloading fonttools-4.57.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->nnunetv2)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->nnunetv2)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas->nnunetv2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas->nnunetv2) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->nnunetv2) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests->nnunetv2) (2025.4.26)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->nnunetv2)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting argparse (from unittest2->batchgenerators>=0.25.1->nnunetv2)\n",
      "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting traceback2 (from unittest2->batchgenerators>=0.25.1->nnunetv2)\n",
      "  Downloading traceback2-1.4.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting linecache2 (from traceback2->unittest2->batchgenerators>=0.25.1->nnunetv2)\n",
      "  Downloading linecache2-1.0.0-py2.py3-none-any.whl.metadata (1000 bytes)\n",
      "Collecting PyYAML (from yacs->nnunetv2)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading blosc2-3.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_image-0.25.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading simpleitk-2.5.0-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tifffile-2025.3.30-py3-none-any.whl (226 kB)\n",
      "Downloading connected_components_3d-3.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dicom2nifti-2.6.1-py3-none-any.whl (43 kB)\n",
      "Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading fft_conv_pytorch-1.2.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Downloading imagecodecs-2025.3.30-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.57.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Downloading msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (401 kB)\n",
      "Downloading ndindex-1.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (521 kB)\n",
      "Downloading nibabel-5.3.2-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numexpr-2.10.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (400 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading python_gdcm-3.0.24.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n",
      "Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n",
      "Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: nnunetv2, acvl-utils, dynamic-network-architectures, batchgenerators, batchgeneratorsv2\n",
      "  Building wheel for nnunetv2 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nnunetv2: filename=nnunetv2-2.6.0-py3-none-any.whl size=277012 sha256=97138b60a47e0f00bcf35fba71bd7202b7ad4c387ce9d2ea70461c53b6a2a9cf\n",
      "  Stored in directory: /home/onyxia/.cache/pip/wheels/7a/2a/8e/4a6e8b1ea9687bf2d37d914be42c6403a6c868d4ae547aafb4\n",
      "  Building wheel for acvl-utils (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for acvl-utils: filename=acvl_utils-0.2.5-py3-none-any.whl size=27242 sha256=472134c96b1062de91f3de0e49bcb9a376556ac44b82349aace5585c96c1adfe\n",
      "  Stored in directory: /home/onyxia/.cache/pip/wheels/70/c4/16/bae888b32a033f634d91a14256a8f8c8ec97db4e4dad8f0216\n",
      "  Building wheel for dynamic-network-architectures (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dynamic-network-architectures: filename=dynamic_network_architectures-0.3.1-py3-none-any.whl size=30108 sha256=49b9afb37edbe46a492e2888f0911da01fbf86c7f883323bd1015b62426f7eed\n",
      "  Stored in directory: /home/onyxia/.cache/pip/wheels/e8/37/e3/40ba582bf18f88191f8700af0ca8bc0a1bff7f8c57b0ebb8df\n",
      "  Building wheel for batchgenerators (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for batchgenerators: filename=batchgenerators-0.25.1-py3-none-any.whl size=93163 sha256=57902f1148a361268afa00d2c1b39738be450d2e501f5880862c0e35ae85d4e2\n",
      "  Stored in directory: /home/onyxia/.cache/pip/wheels/28/21/2b/7b25080f9f5847e6c3162b89d859d7cec9f3093158e56bd008\n",
      "  Building wheel for batchgeneratorsv2 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for batchgeneratorsv2: filename=batchgeneratorsv2-0.2.3-py3-none-any.whl size=47565 sha256=a594390a5b1424053d923019d3a10a2b676a736f0be64e18367b682d62f82e9d\n",
      "  Stored in directory: /home/onyxia/.cache/pip/wheels/72/e2/e8/887e13951396f1ee072d29337e5cd59c637b4bff20d5e038e7\n",
      "Successfully built nnunetv2 acvl-utils dynamic-network-architectures batchgenerators batchgeneratorsv2\n",
      "Installing collected packages: SimpleITK, py-cpuinfo, linecache2, argparse, traceback2, tqdm, tifffile, threadpoolctl, scipy, PyYAML, python-gdcm, pyparsing, pydicom, numexpr, nibabel, ndindex, msgpack, lazy-loader, kiwisolver, joblib, imageio, imagecodecs, graphviz, future, fonttools, einops, cycler, contourpy, connected-components-3d, yacs, unittest2, scikit-learn, scikit-image, matplotlib, dicom2nifti, blosc2, seaborn, batchgenerators, fft-conv-pytorch, dynamic-network-architectures, acvl-utils, batchgeneratorsv2, nnunetv2\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43/43\u001b[0m [nnunetv2]nnunetv2]acvl-utils]tors]ents-3d]\n",
      "\u001b[1A\u001b[2KSuccessfully installed PyYAML-6.0.2 SimpleITK-2.5.0 acvl-utils-0.2.5 argparse-1.4.0 batchgenerators-0.25.1 batchgeneratorsv2-0.2.3 blosc2-3.3.2 connected-components-3d-3.23.0 contourpy-1.3.2 cycler-0.12.1 dicom2nifti-2.6.1 dynamic-network-architectures-0.3.1 einops-0.8.1 fft-conv-pytorch-1.2.0 fonttools-4.57.0 future-1.0.0 graphviz-0.20.3 imagecodecs-2025.3.30 imageio-2.37.0 joblib-1.4.2 kiwisolver-1.4.8 lazy-loader-0.4 linecache2-1.0.0 matplotlib-3.10.1 msgpack-1.1.0 ndindex-1.9.2 nibabel-5.3.2 nnunetv2-2.6.0 numexpr-2.10.2 py-cpuinfo-9.0.0 pydicom-3.0.1 pyparsing-3.2.3 python-gdcm-3.0.24.1 scikit-image-0.25.2 scikit-learn-1.6.1 scipy-1.15.2 seaborn-0.13.2 threadpoolctl-3.6.0 tifffile-2025.3.30 tqdm-4.67.1 traceback2-1.4.0 unittest2-1.1.0 yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "!pip install nnunetv2 tqdm s3fs\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import s3fs\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff484b16",
   "metadata": {},
   "source": [
    "Before training the models, you need to enter your credentials. Example: email --> blabla.blabla@ensae.fr, name --> username Onyxia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc69f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git configured with email : leo.leroy@ensae.fr and username : leoacpr\n"
     ]
    }
   ],
   "source": [
    "email = input(\"Enter your email ENSAE: \")\n",
    "name = input(\"Enter your username Onyxia: \")\n",
    "\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", email])\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", name])\n",
    "\n",
    "print(f\"Git configured with email : {email} and username : {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20327ed2",
   "metadata": {},
   "source": [
    "Now, you must enter your S3 private keys. They are available on Onyxia > Account > Connexion au stockage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef67856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS keys configured as environment variables.\n"
     ]
    }
   ],
   "source": [
    "aws_access_key_id = input(\"Enter your AWS_ACCESS_KEY_ID: \")\n",
    "aws_secret_access_key = input(\"Enter your AWS_SECRET_ACCESS_KEY: \")\n",
    "aws_session_token = input(\"Enter your AWS_SESSION_TOKEN: \")\n",
    "\n",
    "# Environment variables\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = aws_access_key_id\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = aws_secret_access_key\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = aws_session_token\n",
    "\n",
    "print(\"AWS keys configured as environment variables.\")\n",
    "\n",
    "#a été géré avec les fichiers secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3897125b",
   "metadata": {},
   "source": [
    "## 2. Downloading files from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfde054",
   "metadata": {},
   "source": [
    "The datasets are stored on the S3 service provided by Onyxia. The are available on the path   ``projet-statapp-segmedic/diffusion``. You need to download them locally by running the code below. Estimated time: 4 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b74a21c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion to  MinIO S3 Onyxia\n",
    "s3 = s3fs.S3FileSystem(\n",
    "    client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},\n",
    "    key=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    secret=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    token=os.getenv(\"AWS_SESSION_TOKEN\")\n",
    ")\n",
    "#print(len(s3.ls(\"projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_raw/Dataset001_Annot1/labelsTr\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9254a8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Téléchargement du dossier nnUNet_raw...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fichiers dans nnUNet_raw: 100%|██████████| 133/133 [03:22<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Téléchargement du dossier nnUNet_preprocessed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fichiers dans nnUNet_preprocessed: 100%|██████████| 253/253 [03:25<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Téléchargement du dossier nnUNet_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fichiers dans nnUNet_results: 100%|██████████| 55/55 [00:43<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration finished. Environment variables created:\n",
      "nnUNet_raw=/tmp/nnunet/nnUNet_raw\n",
      "nnUNet_preprocessed=/tmp/nnunet/nnUNet_preprocessed\n",
      "nnUNet_results=/tmp/nnunet/nnUNet_results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def download_s3_folder():\n",
    "    \n",
    "    # Defining paths\n",
    "    base_local_path = Path('/tmp/nnunet')\n",
    "    s3_base_path = \"projet-statapp-segmedic/diffusion/nnunet_dataset\"\n",
    "    folders = ['nnUNet_raw', 'nnUNet_preprocessed', 'nnUNet_results']\n",
    "    \n",
    "    # Creating local folders\n",
    "    for folder in folders:\n",
    "        local_folder = base_local_path / folder\n",
    "        local_folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        s3_path = f\"{s3_base_path}/{folder}\"\n",
    "        print(f\"\\nTéléchargement du dossier {folder}...\")\n",
    "        \n",
    "        # Recursive list of all files from S3\n",
    "        try:\n",
    "            files = s3.find(s3_path)\n",
    "            \n",
    "            # Progression bar (very nice!)\n",
    "            with tqdm(total=len(files), desc=f\"Fichiers dans {folder}\") as pbar:\n",
    "                for file_path in files:\n",
    "                    relative_path = file_path.replace(s3_path, '').lstrip('/')\n",
    "                    local_file_path = local_folder / relative_path\n",
    "                    \n",
    "                    # Creating local files if needed\n",
    "                    local_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    \n",
    "                    # Dowloading files\n",
    "                    if not local_file_path.exists():\n",
    "                        try:\n",
    "                            s3.get(file_path, str(local_file_path))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error while downloading {file_path}: {e}\")\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while reading {s3_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        #ERROR CORRECTED: the nnU-Net dataset naming convention requires 4 digit for image case file, not 3. \n",
    "        for string in ['1', '2', '3']:\n",
    "            images = Path(f\"/tmp/nnunet/nnUNet_raw/Dataset00{string}_Annot{string}/imagesTr\")\n",
    "            for f in images.glob(\"*_000.nii.gz\"):\n",
    "                f.rename(f.with_name(f.name.replace(\"_000.nii.gz\", \"_0000.nii.gz\")))\n",
    "    \n",
    "    # Creating global variables for paths, needed for nnU-Net training. \n",
    "    env_vars = {\n",
    "        'nnUNet_raw': str(base_local_path / 'nnUNet_raw'),\n",
    "        'nnUNet_preprocessed': str(base_local_path / 'nnUNet_preprocessed'),\n",
    "        'nnUNet_results': str(base_local_path / 'nnUNet_results')\n",
    "    }\n",
    "    \n",
    "    for var_name, path in env_vars.items():\n",
    "        os.environ[var_name] = path\n",
    "    \n",
    "    # Adding to .bashrc\n",
    "    with open(os.path.expanduser('~/.bashrc'), 'a') as f:\n",
    "        f.write('\\n# nnUNet paths\\n')\n",
    "        for var_name, path in env_vars.items():\n",
    "            f.write(f'export {var_name}=\"{path}\"\\n')\n",
    "    \n",
    "    print(\"\\nConfiguration finished. Environment variables created:\")\n",
    "    for var_name, path in env_vars.items():\n",
    "        print(f\"{var_name}={path}\")\n",
    "\n",
    "    #To apply changes:\n",
    "    !source ~/.bashrc\n",
    "\n",
    "download_s3_folder()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd7a23",
   "metadata": {},
   "source": [
    "Verify if the downloading has been done successfully by running the following line. \n",
    "\n",
    "Expected output: _dataset_fingerprint.json gt_segmentations nnUNetPlans.json dataset.json nnUNetPlans_3d_fullres splits_final.json_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d07893d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_fingerprint.json  gt_segmentations\t  nnUNetPlans.json\n",
      "dataset.json\t\t  nnUNetPlans_3d_fullres  splits_final.json\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/nnunet/nnUNet_preprocessed/Dataset002_Annot2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986f4c7",
   "metadata": {},
   "source": [
    "If you wish to preprocess and verify the datasets integrity, copy-paste and run the following lines. **BE CAREFUL:** this might make Onyxia crash if you do not increase the CPU and RAM ressources! It also takes more than 20 min per line.The lines have already been run before. You normally do not need to run them. That is why the lines are not in a code cell.\n",
    "\n",
    "``!nnUNetv2_plan_and_preprocess -h``\n",
    "\n",
    "``!nnUNetv2_plan_and_preprocess -d 1_Annot1 -c 3d_fullres --verify_dataset_integrity -np 2 -npfp 2``\n",
    "\n",
    "``!nnUNetv2_plan_and_preprocess -d 2 -c 3d_fullres --verify_dataset_integrity -np 2 -npfp 2``\n",
    "\n",
    "``!nnUNetv2_plan_and_preprocess -d 3 -c 3d_fullres --verify_dataset_integrity -np 2 -npfp 2``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da6e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nnUNetv2_plan_and_preprocess: command not found\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'upload_to_s3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mnnUNetv2_plan_and_preprocess -d 3 -c 3d_fullres --verify_dataset_integrity -np 2 -npfp 2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mupload_to_s3\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mnnUNet_preprocessed\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'upload_to_s3' is not defined"
     ]
    }
   ],
   "source": [
    "!nnUNetv2_plan_and_preprocess -d 3 -c 3d_fullres --verify_dataset_integrity -np 2 -npfp 2\n",
    "upload_to_s3(\"nnUNet_preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b1466",
   "metadata": {},
   "source": [
    "(**Optional**) If you wish to upload all the documents stored locally, you can run the following code. Select one folder among ``nnUNet_preprocessed`` or ``nnUNet_results`` (you normally do not need to upload files from nnUNet_raw). Estimated time: between 10s and 1min10s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1b78d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(folder):\n",
    "    # Dossier local et distant\n",
    "    local_folder = Path(f'/tmp/nnunet/{folder}')\n",
    "    s3_folder = f\"projet-statapp-segmedic/diffusion/nnunet_dataset/{folder}\"\n",
    "    \n",
    "    # Lister tous les fichiers à uploader\n",
    "    files = list(local_folder.rglob(\"*\"))\n",
    "    \n",
    "    print(f\"\\nUploading {folder} to {s3_folder}...\")\n",
    "    with tqdm(total=len(files), desc=f\"Upload {folder}\") as pbar:\n",
    "        for file_path in files:\n",
    "            if file_path.is_file():\n",
    "                relative_path = file_path.relative_to(local_folder)\n",
    "                s3_path = f\"{s3_folder}/{relative_path.as_posix()}\"\n",
    "                try:\n",
    "                    s3.put(str(file_path), s3_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lors de l'upload de {file_path} → {s3_path}: {e}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "#upload_to_s3(input(\"Enter nnUNet_preprocessed or nnUNet_results\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f83170",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0b2f6",
   "metadata": {},
   "source": [
    "Training must be jointly done with file uploading: The training creates many documents to save progress. These documents are stored locally, but we need them on S3. Given that epochs take usually about 200s, I decided to set the time interval of uploading to 200s.\n",
    "\n",
    "Decide on which dataset (i.e. which set of annotations) you want to use: ``Dataset001_Annot1``, ``Dataset002_Annot2``, ``Dataset003_Annot3``. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebb6190",
   "metadata": {},
   "source": [
    "**CAREFUL**: The project isn't entirely done. For the moment, there is no early stopping. You should continuoulsy check if Onyxia hasn't crashed during the training (normally it shouldn't happen) and stop about 80 epochs. If you wish to resume training, you can enter this: ``nnUNetv2_train <dataset> 3d_fullres all --npz --c`` but it will only resume from a multiple of 50 epochs (nnU-Net automatically saves its results every 50 epochs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208da390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Uploader] Starting S3 sync thread.\n",
      "\n",
      "Uploading nnUNet_results to projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_results...\n",
      "[Trainer] Launching nnUNet training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results:  26%|██▌       | 17/66 [00:09<00:29,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-05-02 11:45:12.836518: Using torch.compile...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results:  44%|████▍     | 29/66 [00:15<00:09,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-02 11:45:17.018132: do_dummy_2d_data_aug: False\n",
      "using pin_memory on device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results:  48%|████▊     | 32/66 [00:19<00:22,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pin_memory on device 0\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [192, 112, 112], 'median_image_size_in_voxels': [943.0, 512.0, 512.0], 'spacing': [1.0, 0.9626015722751617, 0.9626015722751617], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 1, 1]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset002_Annot2', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 0.9626015722751617, 0.9626015722751617], 'original_median_shape_after_transp': [956, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 901.9999389648438, 'mean': 116.67008209228516, 'median': 113.99999237060547, 'min': -1036.0001220703125, 'percentile_00_5': -34.99999237060547, 'percentile_99_5': 255.0, 'std': 43.83061599731445}}} \n",
      "\n",
      "2025-05-02 11:45:22.695599: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-05-02 11:45:22.753575: \n",
      "2025-05-02 11:45:22.770849: Epoch 1\n",
      "2025-05-02 11:45:22.771125: Current learning rate: 0.00999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results: 100%|██████████| 66/66 [00:42<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload done\n",
      "2025-05-02 11:49:14.397983: train_loss -0.0339\n",
      "2025-05-02 11:49:14.399212: val_loss -0.0221\n",
      "2025-05-02 11:49:14.399558: Pseudo dice [np.float32(0.0), np.float32(0.5369), np.float32(0.7303)]\n",
      "2025-05-02 11:49:14.399880: Epoch time: 231.65 s\n",
      "2025-05-02 11:49:14.400104: Yayy! New best EMA pseudo Dice: 0.38679999113082886\n",
      "2025-05-02 11:49:18.646526: \n",
      "2025-05-02 11:49:18.647278: Epoch 2\n",
      "2025-05-02 11:49:18.647652: Current learning rate: 0.00998\n",
      "\n",
      "Uploading nnUNet_results to projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results: 100%|██████████| 67/67 [00:40<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload done\n",
      "2025-05-02 11:52:34.282944: train_loss -0.1454\n",
      "2025-05-02 11:52:34.283978: val_loss -0.2414\n",
      "2025-05-02 11:52:34.284546: Pseudo dice [np.float32(0.2969), np.float32(0.5653), np.float32(0.8719)]\n",
      "2025-05-02 11:52:34.284927: Epoch time: 195.64 s\n",
      "2025-05-02 11:52:34.285168: Yayy! New best EMA pseudo Dice: 0.4059000015258789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-16:\n",
      "Process Process-17:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 363, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/queues.py\", line 219, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/usr/local/lib/python3.12/threading.py\", line 1149, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/local/lib/python3.12/threading.py\", line 1169, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Process Process-14:\n",
      "Process Process-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 363, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/queues.py\", line 219, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/usr/local/lib/python3.12/threading.py\", line 1149, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/local/lib/python3.12/threading.py\", line 1169, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 363, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/queues.py\", line 219, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/usr/local/lib/python3.12/threading.py\", line 1149, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/local/lib/python3.12/threading.py\", line 1169, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Process Process-13:\n",
      "Traceback (most recent call last):\n",
      "Process Process-7:\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 363, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/queues.py\", line 219, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/usr/local/lib/python3.12/threading.py\", line 1149, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/local/lib/python3.12/threading.py\", line 1169, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.12/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py\", line 41, in producer\n",
      "    with threadpool_limits(1, None):\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/threadpoolctl.py\", line 593, in __exit__\n",
      "    self.restore_original_limits()\n",
      "  File \"/usr/local/lib/python3.12/site-packages/threadpoolctl.py\", line 607, in restore_original_limits\n",
      "    lib_controller.set_num_threads(original_info[\"num_threads\"])\n",
      "  File \"/usr/local/lib/python3.12/site-packages/threadpoolctl.py\", line 497, in set_num_threads\n",
      "    def set_num_threads(self, num_threads):\n",
      "    \n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 335, in _exit_function\n",
      "    info('process shutting down')\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 52, in info\n",
      "    def info(msg, *args):\n",
      "    \n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m uploader_thread.start()\n\u001b[32m     38\u001b[39m trainer_thread.start()\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mtrainer_thread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[Main] All done.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/threading.py:1149\u001b[39m, in \u001b[36mThread.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1146\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot join current thread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1149\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1151\u001b[39m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[32m   1152\u001b[39m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[32m   1153\u001b[39m     \u001b[38;5;28mself\u001b[39m._wait_for_tstate_lock(timeout=\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/threading.py:1169\u001b[39m, in \u001b[36mThread._wait_for_tstate_lock\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1169\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1170\u001b[39m         lock.release()\n\u001b[32m   1171\u001b[39m         \u001b[38;5;28mself\u001b[39m._stop()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/nnUNetv2_train\", line 8, in <module>\n",
      "    sys.exit(run_training_entry())\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/nnunetv2/run/run_training.py\", line 267, in run_training_entry\n",
      "    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,\n",
      "  File \"/usr/local/lib/python3.12/site-packages/nnunetv2/run/run_training.py\", line 207, in run_training\n",
      "    nnunet_trainer.run_training()\n",
      "  File \"/usr/local/lib/python3.12/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py\", line 1381, in run_training\n",
      "    self.on_epoch_end()\n",
      "  File \"/usr/local/lib/python3.12/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py\", line 1142, in on_epoch_end\n",
      "    self.save_checkpoint(join(self.output_folder, 'checkpoint_best.pth'))\n",
      "  File \"/usr/local/lib/python3.12/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py\", line 1170, in save_checkpoint\n",
      "    torch.save(checkpoint, filename)\n",
      "  File \"/usr/local/lib/python3.12/site-packages/torch/serialization.py\", line 965, in save\n",
      "    _save(\n",
      "  File \"/usr/local/lib/python3.12/site-packages/torch/serialization.py\", line 1266, in _save\n",
      "    zip_file.write_record(name, storage, num_bytes)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trainer] Training complete.\n",
      "\n",
      "Uploading nnUNet_results to projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results: 100%|██████████| 67/67 [00:21<00:00,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading nnUNet_results to projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results: 100%|██████████| 67/67 [00:35<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading nnUNet_results to projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results: 100%|██████████| 67/67 [00:18<00:00,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading nnUNet_results to projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results: 100%|██████████| 67/67 [00:20<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading nnUNet_results to projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results: 100%|██████████| 67/67 [00:20<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading nnUNet_results to projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results: 100%|██████████| 67/67 [00:22<00:00,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading nnUNet_results to projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results: 100%|██████████| 67/67 [00:18<00:00,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading nnUNet_results to projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results: 100%|██████████| 67/67 [00:31<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading nnUNet_results to projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results: 100%|██████████| 67/67 [00:28<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading nnUNet_results to projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results: 100%|██████████| 67/67 [00:27<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading nnUNet_results to projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results: 100%|██████████| 67/67 [00:33<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading nnUNet_results to projet-statapp-segmedic/diffusion/nnunet_dataset/nnUNet_results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload nnUNet_results: 100%|██████████| 67/67 [00:20<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Code to train and upload nnU-Net\n",
    "\n",
    "dataset=input(\"Enter one among: Dataset001_Annot1, Dataset002_Annot2, Dataset003_Annot3\")\n",
    "\n",
    "# Upload function with time interval \n",
    "# IDEA: upload as soon as the content of temp/results changes\n",
    "def sync_results_to_s3():\n",
    "    print(\"[Uploader] Starting S3 sync thread.\")\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        upload_to_s3('nnUNet_results')\n",
    "        print(\"upload done\")\n",
    "\n",
    "        time.sleep(300)\n",
    "\n",
    "\n",
    "# Training function\n",
    "def run_training():\n",
    "    print(\"[Trainer] Launching nnUNet training...\")\n",
    "    command = [\n",
    "        \"nnUNetv2_train\",\n",
    "        f\"{dataset}\",  # Dataset ID\n",
    "        \"3d_fullres\",  # Plan\n",
    "        \"all\",  # Fold            \n",
    "        \"--npz\",\n",
    "        \"--c\"\n",
    "    ]\n",
    "    subprocess.run(command)\n",
    "    print(\"[Trainer] Training complete.\")\n",
    "\n",
    "\n",
    "# Threads\n",
    "uploader_thread = threading.Thread(target=sync_results_to_s3, daemon=True)\n",
    "trainer_thread = threading.Thread(target=run_training)\n",
    "\n",
    "uploader_thread.start()\n",
    "trainer_thread.start()\n",
    "\n",
    "trainer_thread.join()\n",
    "print(\"[Main] All done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b97d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33596f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
